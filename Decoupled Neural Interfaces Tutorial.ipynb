{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoupled Neural Interfaces\n",
    "\n",
    "An implementation of DNIs in TensorFlow. As in the referenced paper, the feasability of this technique is demonstrated through the use of stochastic layer-wise updates when training a fully connected network on the MNIST classification problem.\n",
    "\n",
    "Reference: [Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/abs/1608.05343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tqdm import tqdm # Used to display training progress bar\n",
    "\n",
    "sg_sess = tf.Session()\n",
    "backprop_sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "Architecture and training largely follow the details provided in section **A.1 Feed-Forward Implementation Details** of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data scaled to [0,1] interval, and labels in one-hot format\n",
    "# 55k train, 5k validation, 10k test\n",
    "MNIST = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "iterations = 500000\n",
    "batch_size = 250 # modified to evenly divide dataset size\n",
    "\n",
    "init_lr = 3e-5\n",
    "lr_div = 10\n",
    "lr_div_steps = set([300000, 400000])\n",
    "\n",
    "update_prob = 0.2 # Probability of updating a decoupled layer\n",
    "\n",
    "validation_checkpoint = 10 # How often (iterations) to validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions for constructing layers\n",
    "def dense_layer(inputs, units, name, output=False):\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.dense(inputs, units, name=\"fc\")\n",
    "        if not output:\n",
    "            x = tf.layers.batch_normalization(x, name=\"bn\")\n",
    "            x = tf.nn.relu(x, name=\"relu\")\n",
    "    return x\n",
    "\n",
    "def sg_module(inputs, units, name, label):\n",
    "    with tf.variable_scope(name):\n",
    "        inputs_c = tf.concat([inputs, label], 1)\n",
    "        x = tf.layers.dense(inputs_c, units, name=\"fc\", kernel_initializer=tf.zeros_initializer())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ops for network architecture\n",
    "with tf.variable_scope(\"architecture\"):\n",
    "    # Inputs\n",
    "    with tf.variable_scope(\"input\"):\n",
    "        X = tf.placeholder(tf.float32, shape=(None, 784), name=\"data\") # Input\n",
    "        Y = tf.placeholder(tf.float32, shape=(None, 10), name=\"labels\") # Target\n",
    "    \n",
    "    # Inference layers\n",
    "    h1 = dense_layer(X, 256, \"layer1\")\n",
    "    h2 = dense_layer(h1, 256, name=\"layer2\")\n",
    "    h3 = dense_layer(h2, 256, name=\"layer3\")\n",
    "    logits = dense_layer(h3, 10, name=\"layer4\", output=True)\n",
    "    \n",
    "    # Synthetic Gradient layers\n",
    "    d1_hat = sg_module(h1, 256, \"sg2\", Y)\n",
    "    d2_hat = sg_module(h2, 256, \"sg3\", Y)\n",
    "    d3_hat = sg_module(h3, 256, \"sg4\", Y)\n",
    "\n",
    "# Collections of trainable variables in each block\n",
    "layer_vars = [tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer1/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer2/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer3/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer4/\")]\n",
    "sg_vars = [None,\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg2/\"),\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg3/\"),\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg4/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for optimizing a layer and its synthetic gradient module\n",
    "def train_layer_n(n, h_m, h_n, d_hat_m, class_loss, d_n=None):\n",
    "    with tf.variable_scope(\"layer\"+str(n)):\n",
    "        layer_grads = tf.gradients(h_n, [h_m]+layer_vars[n-1], d_n)\n",
    "        layer_gv = list(zip(layer_grads[1:],layer_vars[n-1]))\n",
    "        layer_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).apply_gradients(layer_gv)\n",
    "    with tf.variable_scope(\"sg\"+str(n)):\n",
    "        d_m = layer_grads[0]\n",
    "        sg_loss = tf.divide(tf.losses.mean_squared_error(d_hat_m, d_m), class_loss)\n",
    "        sg_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(sg_loss, var_list=sg_vars[n-1])\n",
    "    return layer_opt, sg_opt\n",
    "\n",
    "# Ops for training\n",
    "with tf.variable_scope(\"train\"):\n",
    "    with tf.variable_scope(\"learning_rate\"):\n",
    "        learning_rate = tf.Variable(init_lr, dtype=tf.float32, name=\"lr\")\n",
    "        reduce_lr = tf.assign(learning_rate, learning_rate/lr_div, name=\"lr_decrease\")\n",
    "\n",
    "    pred_loss = tf.losses.softmax_cross_entropy(onehot_labels=Y, logits=logits, scope=\"prediction_loss\")\n",
    "    \n",
    "    # Optimizers when using synthetic gradients\n",
    "    with tf.variable_scope(\"synthetic\"):\n",
    "        layer4_opt, sg4_opt = train_layer_n(4, h3, pred_loss, d3_hat, pred_loss)\n",
    "        layer3_opt, sg3_opt = train_layer_n(3, h2, h3, d2_hat, pred_loss, d3_hat)\n",
    "        layer2_opt, sg2_opt = train_layer_n(2, h1, h2, d1_hat, pred_loss, d2_hat)\n",
    "        with tf.variable_scope(\"layer1\"):\n",
    "            layer1_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(h1, var_list=layer_vars[0], grad_loss=d1_hat)\n",
    "    \n",
    "    # Optimizer when using backprop\n",
    "    with tf.variable_scope(\"backprop\"):\n",
    "        backprop_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(pred_loss)\n",
    "        \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ops for validation and testing (computing classification accuracy)\n",
    "with tf.variable_scope(\"test\"):\n",
    "    preds = tf.nn.softmax(logits, name=\"predictions\")\n",
    "    correct_preds = tf.equal(tf.argmax(preds,1), tf.argmax(Y,1), name=\"correct_predictions\")\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds,tf.float32), name=\"correct_prediction_count\") / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ops for tensorboard summary data\n",
    "with tf.variable_scope(\"summary\"):\n",
    "    cost_summary_opt = tf.summary.scalar(\"loss\", pred_loss)\n",
    "    accuracy_summary_opt = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "With training using backprop, each layer cannot be updated until the gradient from the loss is calculated for the following layer. This update-locking is avoided when using synthetic gradients, as the next layer's gradients are approximated by doing a forward pass through that layer's synthetic gradient module. However, synthetic gradient modules themselves are update-locked to the layer they are grouped with, as updating the gradient estimation will require the true gradient which we only calculate when updating the layer itself. Though not used as such here, the benefit of using synthetic gradients is that it allows training of layers to be parallelized across many GPUs, which would be a significant speedup when training large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 500000/500000 [58:37<00:00, 160.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train using backprop as benchmark\n",
    "with backprop_sess.as_default():\n",
    "    backprop_train_writer = tf.summary.FileWriter(\"logging/backprop/train\")\n",
    "    backprop_validation_writer = tf.summary.FileWriter(\"logging/backprop/validation\")\n",
    "\n",
    "    backprop_sess.run(init)\n",
    "    for i in tqdm(range(1,iterations+1)):\n",
    "        if i in lr_div_steps: # Decrease learning rate\n",
    "            backprop_sess.run(reduce_lr)\n",
    "        \n",
    "        data, target = MNIST.train.next_batch(batch_size)\n",
    "        _, summary = backprop_sess.run([backprop_opt, summary_op], feed_dict={X:data,Y:target})\n",
    "        backprop_train_writer.add_summary(summary, i)\n",
    "        \n",
    "        if i % validation_checkpoint == 0:\n",
    "            Xb, Yb = MNIST.test.next_batch(batch_size)\n",
    "            summary = backprop_sess.run([summary_op], feed_dict={X:Xb,Y:Yb})[0]\n",
    "            backprop_validation_writer.add_summary(summary, i)\n",
    "\n",
    "    # Cleanup summary writers\n",
    "    backprop_train_writer.close()\n",
    "    backprop_validation_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 500000/500000 [37:46<00:00, 220.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train using synthetic gradients\n",
    "with sg_sess.as_default():\n",
    "    sg_train_writer = tf.summary.FileWriter(\"logging/sg/train\", sg_sess.graph)\n",
    "    sg_validation_writer = tf.summary.FileWriter(\"logging/sg/validation\")\n",
    "    \n",
    "    sg_sess.run(init)\n",
    "    for i in tqdm(range(1,iterations+1)):\n",
    "        if i in lr_div_steps: # Decrease learning rate\n",
    "            sg_sess.run(reduce_lr)\n",
    "        \n",
    "        data, target = MNIST.train.next_batch(batch_size)\n",
    "        \n",
    "        # Each layer can now be independently updated (could be parallelized)\n",
    "        if random.random() <= update_prob: # Stochastic updates are possible\n",
    "            sg_sess.run([layer1_opt], feed_dict={X:data,Y:target})\n",
    "        if random.random() <= update_prob:\n",
    "            sg_sess.run([layer2_opt, sg2_opt], feed_dict={X:data,Y:target})\n",
    "        if random.random() <= update_prob:\n",
    "            sg_sess.run([layer3_opt, sg3_opt], feed_dict={X:data,Y:target})\n",
    "        if random.random() <= update_prob:\n",
    "            _, _, summary = sg_sess.run([layer4_opt, sg4_opt, summary_op], feed_dict={X:data,Y:target})\n",
    "            sg_train_writer.add_summary(summary, i)\n",
    "        \n",
    "        if i % validation_checkpoint == 0:\n",
    "            Xb, Yb = MNIST.test.next_batch(batch_size)\n",
    "            summary = sg_sess.run([summary_op], feed_dict={X:Xb,Y:Yb})[0]\n",
    "            sg_validation_writer.add_summary(summary, i)\n",
    "    \n",
    "    # Cleanup summary writers\n",
    "    sg_train_writer.close()\n",
    "    sg_validation_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphed on TensorBoard, we see that the stochastically updated network manages to reach the validation accuracy of the network trained using backprop:  \n",
    "\n",
    "Orange - backprop/train  \n",
    "Light Blue - backprop/validate  \n",
    "Purple - sg/train  \n",
    "Dark Blue - sg/validate\n",
    "\n",
    "![Accuracy](images/dni_accuracy.png) ![Loss](images/dni_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.351406607963\n",
      "0.976400002837\n"
     ]
    }
   ],
   "source": [
    "# Test using backprop\n",
    "with backprop_sess.as_default():\n",
    "    n_batches = int(MNIST.test.num_examples/batch_size)\n",
    "    test_accuracy = 0\n",
    "    test_loss = 0\n",
    "    for _ in range(n_batches):\n",
    "        Xb, Yb = MNIST.test.next_batch(batch_size)\n",
    "        batch_accuracy, batch_loss = backprop_sess.run([accuracy, pred_loss], feed_dict={X:Xb,Y:Yb})\n",
    "        test_loss += batch_loss\n",
    "        test_accuracy += batch_accuracy\n",
    "    print (test_loss/n_batches)\n",
    "    print(test_accuracy/n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0989040184766\n",
      "0.969500003755\n"
     ]
    }
   ],
   "source": [
    "# Test using synthetic gradients\n",
    "with sg_sess.as_default():\n",
    "    n_batches = int(MNIST.test.num_examples/batch_size)\n",
    "    test_accuracy = 0\n",
    "    test_loss = 0\n",
    "    for _ in range(n_batches):\n",
    "        Xb, Yb = MNIST.test.next_batch(batch_size)\n",
    "        batch_accuracy, batch_loss = sg_sess.run([accuracy, pred_loss], feed_dict={X:Xb,Y:Yb})\n",
    "        test_loss += batch_loss\n",
    "        test_accuracy += batch_accuracy\n",
    "    print (test_loss/n_batches)\n",
    "    print(test_accuracy/n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "sg_sess.close()\n",
    "backprop_sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
